{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch RL - DQN(cartpole)"
      ],
      "metadata": {
        "id": "S7_hzvSPXSe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install gymnasium[classic_control]"
      ],
      "metadata": {
        "id": "RXo3cx3pXXaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend() # interactive python 백엔드인지 확인하는 코드\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion() # plot이 다이나믹하게 matplotlib과 상호작용 할 수 있게 해주는 설정\n",
        "\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "qrlo0r7-VFNM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE2D8xx2WL_C",
        "outputId": "43fe0e7a-5582-4805-bc9c-96762bafcca7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Memory\n",
        "\n",
        "### Replay Memory란?\n",
        "\n",
        "에이전트가 경험하는 관측의 전환(transition)을 저장하고, 나중에 재사용할 수 있게 하는 기법이다. Experience Replay memory에서 무작위로 전환을 추출함으로서, 배치를 구성하는 전환이 서로 자동 상관관계를 갖지 않게 할 수 있다. 이는 DQN이 학습하는 과정을 매우 향상시키는 것으로 알려져 있다.\n",
        "\n",
        "이를 구현하기 위해서는 2개의 클래스가 필요하다.\n",
        "\n",
        "* transition : 환경에서 발생하는 하나의 전환을 표현하는 `named tuple`객체 입니다.\n",
        "* ReplayMemory : 가장 최근에 관측된 전환을 저장하는 주기적인 버퍼입니다. 랜덤 추출을 위해 `.sample()`메소드를 구현해야 합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "RIsDW7dMXpuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Trainsition',\n",
        "                        ('state', 'action', 'next_state', 'reward') # (S_t, a_t, S_{t+1}, r)\n",
        ")"
      ],
      "metadata": {
        "id": "JukeegGeXnuq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "  def __init__(self, capacity):\n",
        "     self.memory = deque([], maxlen=capacity)\n",
        "  def push(self, *args):\n",
        "    \"save a transition\"\n",
        "    self.memory.append(Transition(*args))\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.memory, batch_size)\n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "pm2WhjuwWaHS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DQN 알고리즘\n",
        "\n",
        "Q-learning의 핵심아래와 같다\n",
        "\n",
        "\n",
        " \\begin{align}\n",
        " (1)\\\\\n",
        " Q^* : State\n",
        "\\times Action → R\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "(1)을 통해서 보상이 어떤 형태일지 알 수 있다. 그리고 이를 통해 보상을 최대화하는 정책을 만들어낼 수 있다.\n",
        "\n",
        "\\begin{align}\n",
        "(2)\\\\\n",
        "π^*(s) =  \\rm{arg}\\max_a Q^* (s, a)\n",
        "\\end{align}\n",
        "\n",
        "(2) 하지만 환경 전체를 관측하는 것은 불가능하므로 실제로 우리는 $Q^*$ 함수에 접근 할 수 없다. 하지만 UFA(universal function approximator) 정리를 근거로 우리는 실제  $Q^*$를 근사하는 신경망을 만들어낼 수 있다.\n",
        "\n",
        "\\begin{align}\n",
        "(3)\\\\\n",
        "Q^{π}(s, a) = r + \\gamma Q^{\\pi}( \\acute{s} , π( \\acute{s}))\n",
        "\\end{align}\n",
        "\n",
        "(3) 훈련 업데이트 룰을 위해 특정 정책에 관한 모든 $Q$ 함수는 벨만 방정식을 따른다는 사실을 활용할 것이다.\n",
        "\n",
        "\\begin{align}\n",
        "(4)\\\\\n",
        "\\delta = Q (s, a) - (r + \\gamma \\max_a^{'} Q (\\acute{s}, a))\n",
        "\\end{align}\n",
        "\n",
        "(4)의 등호 양쪽에 있는 차이를 시차 오차라고 한다.\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "(5)\\\\\n",
        "\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
        "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
        "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
        "   \\end{cases}\n",
        "   \\end{align}\n",
        "\n",
        "   (5) 오차를 해결하기 위한 로스 함수로 Huber loss를 사용할 것이다. Huber loss는 오차가 작을 때는 MSE 처럼 동작하지만 오차가 크면 MAE처럼 동작한다. 이는 $Q$ 함수가 매우 많은 노이즈를 가지고 있을 때 아웃라이어에도 견고하게 동작할 수 있게 한다. 이때, 전환의 배치인 B를 분모로 두어서 계산한다.\n",
        "  "
      ],
      "metadata": {
        "id": "oFYwvxhjnW1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-network\n",
        "\n",
        "간단한 feed forward 네크워크를 통해 q-network를 구현할 것이다.\n",
        "* input : 현재 스크린과 이전스크린의 차이\n",
        "*output : Q(s, left), Q(s, right) - 이는 상태에 대한 액션\n"
      ],
      "metadata": {
        "id": "aBlOM8YBzC4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, n_observations, n_actions):\n",
        "    super(DQN, self).__init__()\n",
        "    self.layer1 = nn.Linear(n_observations, 128)\n",
        "    self.layer2 = nn.Linear(128, 128)\n",
        "    self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = F.relu(self.layer2(x))\n",
        "    return self.layer3(x)"
      ],
      "metadata": {
        "id": "jKcHVLcVXNyb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "### 하이퍼파라미터와 유틸리티\n",
        "\n",
        "아래의 셀은 모델과 옵티마이저를 인스턴스화 할 것이다, 그리고 몇가지 도구들을 정의할 것이다.\n",
        "\n",
        "* select_action : 입실론 그리디 정책에 따라 액션을 선택하는 도구이다. 때때로는 모델이 액션을 선택하도록 하고 때때로는 균일하게 하나의 액션을 랜덤하게 고를 것이다. 랜덤한 액션을 고르는 확률은 `EPS_START` 에서 시작해서 지수적으로 `EPS_END`로 감쇠할 것이다. `EPS_ DECAY`는 감쇠의 속도를 제어하는 인자이다.\n",
        "* `plot_duration` :  이전의 100개의 에피소드의 평균값 - eval과정에서의 측정값 - 과 함께 에피소드의 지속을 표시하는 helper입니다. 해당 플롯은 트레이닝 루프가 포함된 셀 아래에 표시되고 에피소드마다 업데이트 됩니다."
      ],
      "metadata": {
        "id": "h98vY9wX2LdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE 는 리플레이 버퍼에서 샘플되는 전환의 숫자입니다.\n",
        "# GAMMA 는 할인율입니다.\n",
        "# EPS_START 입실론의 초깃값입니다.\n",
        "# EPS_END 입실론의 마지막 값입니다.\n",
        "# EPS_DECAY 입실론이 지수적으로 감쇠하는 속도를 제어합니다.\n",
        "# TAU 는 타겟 네트워크를 업데이트 하는 비율입니다.\n",
        "# LR 학습률입니다.\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4"
      ],
      "metadata": {
        "id": "C8tEqigY1msJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 액션의 가짓수. 이산 액션이며 갯수는 2이다.\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "#GYM의 상태 관측 값을 갖고 온다.\n",
        "state, info = env.reset()\n",
        "\n",
        "# state[ 0.03291552  0.02558019 -0.0495155   0.0120102 ], 크기 4의 리스트이다.\n",
        "# info 는 비워져있는 딕트 혹은 셋이다.\n",
        "\n",
        "n_observations = len(state) # 관측의 갯수\n",
        "\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device) # 부록\n",
        "target_net.load_state_dict(policy_net.state_dict()) # 폴리시 네트워크 파라미터를 복사하여 갖고 있는다\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(10000) # replace 메모리 큐의 최대 크기 선언\n",
        "\n",
        "steps_done = 0\n"
      ],
      "metadata": {
        "id": "VDu0O7LdWz7C"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 부록 : 타겟 네트워크가 필요한 이유\n",
        "\n",
        ">Q-learning에서는 추측으로 추측을 업데이트 한다. - from reference -\n",
        "\n",
        "Q-learning의 핵심 아이디어는 Q-value 함수를 반복적으로 업데이트하여 최적의 Q-values를 찾는 것이다. 이건 매우 위험한 상관관계를 잠재적으로 유발할 수 있다. 즉, Q-value 업데이트 중에 나타나는 값들이 서로 상호 의존적이라는 점이다.\n",
        "\n",
        "벨만 방정식은  $Q(s^{'}, a^{'})$를 통해 $Q(s, a)$의 값을 제공한다. s와 s' 사이에는 단 한단계의 차이만 있기 때문에 네트워크가 이를 구분하는 것은 쉽지 않다.\n",
        "\n",
        "이러한 상호 의존성은 학습 과정에서 불안정성을 야기할 수 있다. 예를 들어, Q-value를 업데이트하면서 변화한 값들이 다음 업데이트에 영향을 미치면서 값들이 불규칙하게 수렴하거나 수렴하지 않을 수 있기 때문이다. 이런 문제를 해결하기 위해 타겟 네트워크(target network)라는 개념이 도입되었다.\n",
        "\n",
        "타겟 네트워크는 현재 학습 중인 Q-value 함수와 별개로 존재하는 네트워크다. 주요한 역할은 안정성을 제공하는 것이다. Q-value 업데이트 시에는 타겟 네트워크의 Q-values를 사용하여 업데이트를 수행합니다. 그러나 **실제 학습은 현재의 Q-value 함수**를 기반으로 이루어진다. 이렇게 함으로써 업데이트가 안정적으로 이루어지며, 상호 의존성으로 인한 불안정성을 줄일 수 있다.\n",
        "\n",
        "즉, Q-value 함수를 업데이트하기 위해 타겟 네트워크의 q-value를 사용하여 Q-value 함수의 파라미터를 업데이틑 시키는 것이다.\n",
        "\n",
        "*Reference*\n",
        "\n",
        "[dqn-targetnetwork](https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c)\n",
        "\n"
      ],
      "metadata": {
        "id": "idupNfsKAZpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    #\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold: # 입실론 비율을 넘어서면\n",
        "        with torch.no_grad():\n",
        "          # max(1)을 통해서 각 로우의 가장 큰 칼럼 값이 반환 될 것이다.\n",
        "          # max 메소드를 통해 나온 결과의 두 번째 칼럼은 최대 요소가 존재하던 인덱스이다.\n",
        "          # 따라서 이를 통해 더 커다란 기대 보상을 얻을 수 있다.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "ESgrJlB7AY_r"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episode_durations = []\n",
        "\n",
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드의 평균을 가져와 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ],
      "metadata": {
        "id": "7FHvEudrW1sC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 루프\n",
        "\n",
        "1) 배치 샘플링\n",
        "\n",
        "2) 텐서를 concat 하나로 만듦\n",
        "\n",
        "3) $Q(S_t, a_t)$ 와 $V(S_{t+1}) = \\max_a Q(s_{t+1}, a)$ 계산\n",
        "\n",
        "4) (3)의 값을 결합하여 로스 함수에 넣음\n",
        "\n",
        "5) 정의에 의해, 만약 입실론이 종료 상태에 다다르면 $V(s)= 0$\n",
        "\n",
        "6) 추가적인 안정성을 위해 타겟 네트워크를 사용하여 $V(s_{t+1})$ 계산\n"
      ],
      "metadata": {
        "id": "g6wA9NNpFIDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE) # (1) 배치 샘플링\n",
        "    batch = Transition(*zip(*transitions)) ## 전환을 상태는 상태끼리, 액션은 액션끼리, 다음 상태는 다음 상태 끼리, 보상은 보상끼리 묶어줌\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "pGObOU6vFGx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}