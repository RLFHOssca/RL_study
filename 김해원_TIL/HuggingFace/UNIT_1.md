## UNIT 1 요약 정리

- 강화 학습은 행동을 통해 학습하는 계산적인 접근 방법입니다. 환경과 상호작용하여 시행착오를 겪고 보상(positive or negative)을 피드백으로 받아 에이전트를 구축합니다.
- 모든 강화 학습 에이전트의 목표는 예상 누적 보상(기대 반환값이라고도 함)을 극대화하는 것입니다. 강화 학습은 보상 가설에 기반하여 모든 목표는 기대 누적 보상의 극대화로 설명될 수 있다고 가정합니다.
- 강화 학습 과정은 상태, 행동, 보상 및 다음 상태의 연속을 출력하는 루프입니다.
- 예상 누적 보상(기대 반환값)을 계산하기 위해 우리는 보상을 할인합니다: 보상은 미래의 장기적인 보상보다 초기에 발생하는 보상이 더 예상되기 때문에 할인합니다.
- 강화 학습 문제를 해결하려면 최적 정책을 찾아야 합니다. 정책은 상태가 주어졌을 때 어떤 행동을 취해야 하는지를 알려주는 AI의 "두뇌"입니다. 최적 정책은 기대 반환값을 최대화하는 행동을 제공하는 정책입니다.
- 최적 정책을 찾는 두 가지 방법이 있습니다.
    - 정책 기반 방법(Policy-Based Methods) : 정책을 직접 훈련시키는 방법
    - 가치 기반 방법(Value-Based Methods) : 각 상태에서 에이전트가 얻을 기대 반환값을 알려주는 가치 함수를 훈련하고 이 함수를 사용하여 정책을 정의하는 방법
    - 마지막으로, "Deep RL"에 대해 이야기했습니다. 이는 정책 기반 방법을 사용하여 취할 행동을 추정하거나 상태의 가치를 추정하기 위해 심층 신경망을 도입하는 것을 의미하며, "깊은"이라는 이름이 붙여진 이유입니다.

## 실습

- 훈련된 모델 게시 : https://huggingface.co/54data/ppo-LunarLander-v2
    - mean_reward on LunarLander-v2 : 269.20 +/- 14.89

## Q1: 강화 학습이란 무엇인가요?

- 강화 학습은 환경과 상호작용을 통해 시행착오를 통해 학습하고 보상(positive or negative)을 고유한 피드백으로 받아 환경을 학습하는 에이전트를 구축하여 제어 작업(또는 의사 결정 문제)를 해결하는 프레임워크입니다.

## Q2: 강화 학습 루프를 정의하세요.

각 단계마다:

- 우리의 에이전트는 환경으로부터 상태 $S_{0}$을 받습니다.
- 상태 $S_{0}$을 기반으로 에이전트는 행동 $A_{0}$를 취합니다.
- 우리의 에이전트는 오른쪽으로 이동합니다.
- 환경은 새로운 상태 $S_{1}$로 전환됩니다.
- 환경은 에이전트에게 보상 $R_{1}$을 제공합니다.

## Q3: 상태(state)와 관찰(observation)의 차이점은 무엇인가요?

- **상태** : 세계의 상태를 완전하게 설명하는 것(숨겨진 정보가 없음).
    - 체스 게임에서는 전체 보드 정보에 접근할 수 있으므로 환경으로부터 상태를 받습니다. 다시 말해, 환경은 완전히 관찰 가능합니다.
- **관찰** : 상태의 부분적인 설명입니다.
    - 슈퍼 마리오 브라더스 게임에서는 플레이어 근처의 부분만을 볼 수 있으므로 관찰을 받습니다.
    - 슈퍼 마리오 브라더스 게임에서는 부분적으로 관찰 가능한 환경에 있습니다. 우리는 레벨의 일부분만을 볼 수 있기 때문에 관찰을 받습니다.

## Q4: 태스크(task)란 강화학습 문제의 하나의 인스턴스를 말합니다. 태스크의 두 가지 유형은 무엇인가요?

1. **Episodic task**
    - 에피소드식 태스크는 시작점과 종료점(종단 상태)이 있습니다. 이로써 에피소드가 생성되며, 상태, 행동, 보상 및 새로운 상태의 목록을 만듭니다.
    - 예를 들어 슈퍼 마리오 브라더스 게임을 생각해 보면, 에피소드는 새로운 마리오 레벨이 시작되는 시점에서 시작되며, 플레이어가 죽거나 레벨을 끝까지 클리어할 때 끝납니다.
2. **Continuing tasks**
    - 계속되는 태스크는 무한히 계속되는 태스크입니다(종단 상태 없음). 이 경우, 에이전트는 최상의 행동을 선택하는 법을 배우고 동시에 환경과 상호 작용해야 합니다.
    - 예를 들어, 주식 자동 거래를 수행하는 에이전트를 생각해 보면, 이 작업에서는 시작점과 종단 상태가 없습니다. 에이전트는 우리가 중지하기 전까지 계속 실행됩니다.

## Q5: 탐색/활용 트레이드오프(Exploration/Exploitation Tradeoff)란 무엇인가요?

강화 학습에서는 환경을 탐색하고 우리가 그 환경에 대해 알고 있는 것을 얼마나 활용할지 균형을 맞추어야 합니다.

- **탐색(Exploration)** : 환경을 탐색하며 환경에 대한 더 많은 정보를 얻기 위해 무작위로 행동을 시도하는 것을 의미합니다.
- **활용(Exploitation)** : 알려진 정보를 이용하여 보상을 최대화하는 것을 의미합니다.

## Q6: 정책(Policy)이란 무엇인가요?

정책(π)은 에이전트의 뇌와 같습니다. 이것은 우리가 있는 상태에서 어떤 동작을 취해야 하는지를 알려주는 함수입니다. 따라서 특정 시간에 에이전트의 행동을 정의합니다.

## Q7: 가치 기반 방법(Value-Based Methods)이란 무엇인가요?

- 가치 기반 방법은 강화 학습 문제를 해결하는 주요 접근 방식 중 하나입니다.
- 가치 기반 방법에서는 정책 함수를 학습하는 대신, 상태를 해당 상태에 있는 것으로 예상되는 가치로 매핑하는 가치 함수를 학습합니다.

## Q8: 정책 기반 방법(Policy-Based Methods)이란 무엇인가요?

- 정책 기반 방법에서는 정책 함수를 직접 학습합니다.
- 이 정책 함수는 각 상태를 해당 상태에서 최상의 대응하는 동작으로 매핑합니다. 또는 해당 상태에서 가능한 행동 집합에 대한 확률 분포로 매핑할 수도 있습니다.
